https://www.awseducate.com/signin/SiteLogin

paso 1  actualizar sistema

	sudo apt update
	sudo apt -y upgrade

paso 2	instalar java

	sudo apt install default-jdk
	java -version

paso 3  descargar spark
	cd /opt/
	sudo curl -O https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz

paso 4 descomprimir spark

	sudo tar xvf spark-3.0.1-bin-hadoop2.7.tgz

paso 4 cambiar propietario carpeta spark

	sudo chown ubuntu:ubuntu -R spark-3.0.1-bin-hadoop2.7

paso 5 variables de entorno

	nano ~/.bashrc 


		export SPARK_HOME=/opt/spark-3.0.1-bin-hadoop2.7
		export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

paso 6 aplicar cambios 

	source ~/.bashrc

paso 7 instalar anaconda 
	cd /opt 
	sudo mkdir anaconda/
	sudo chown ubuntu:ubuntu -R anaconda/
	sudo curl -O https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh
	sha256sum Anaconda3-2020.11-Linux-ppc64le.sh
	bash Anaconda3-2020.11-Linux-x86_64.sh
	source ~/.bashrc
	cd /home/ubuntu/ananconda3/bin
	./conda init
	#salir batch
	#inciar
	#conda create --name myspark
	#conda activate myspark


paso 8 agregar jupyter con pyspark

	nano ~/.bashrc 
	
		export PYSPARK_DRIVER_PYTHON=jupyter
		export PYSPARK_DRIVER_PYTHON_OPTS='notebook'

	source ~/.bashrc

paso 9 configurar master
	cd /opt/spark-3.0.1-bin-hadoop2.7/conf
	cp spark-env.sh.template ./spark-env.sh

ss -tunelp | grep 8080



from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession

sc = SparkContext.getOrCreate()
spark = SparkSession(sc)

